{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(torch.nn.Module):\n",
    "    def __init__(self, size_in, size_out, activation):\n",
    "        super(Layer,self).__init__()\n",
    "        self.weights = torch.nn.Parameter(torch.randn(size_in, size_out,requires_grad=True))\n",
    "        self.bias = torch.nn.Parameter(torch.randn(1,size_out,requires_grad=True))\n",
    "        self.activation = activation\n",
    "        \n",
    "    def Forward(self, z_in):\n",
    "        return self.activation(z_in@self.weights + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "forget = Layer(38,15, torch.nn.Sigmoid())\n",
    "loss_func = torch.nn.MSELoss()\n",
    "opt = torch.optim.Adam(forget.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.9230, -1.3775,  0.0349,  0.8350,  0.3566,  0.0389, -1.3132, -0.7851,\n",
      "         -0.8781, -0.8378, -0.6207,  1.2292,  0.0428,  1.5867, -0.5709]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x_in = torch.randn(1,38)\n",
    "y = torch.rand(1,15)\n",
    "print(forget.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.9220, -1.3765,  0.0339,  0.8340,  0.3576,  0.0379, -1.3122, -0.7861,\n",
      "         -0.8791, -0.8388, -0.6216,  1.2285,  0.0438,  1.5877, -0.5719]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "out = forget.Forward(x_in)\n",
    "loss = loss_func(out,y)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "print(forget.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neuro Network \n",
    "It passes some information to forward layers.... and it takes some data and saves it and uses it again... it kinda creates a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, size_in, size_out, size_mem):\n",
    "        super(RNN,self).__init__()\n",
    "        self.size_mem = size_mem\n",
    "        self.mem_layer = Layer(size_in + size_mem, size_mem, torch.tanh)\n",
    "        self.out_layer = Layer(size_mem, size_out, torch.sigmoid)\n",
    "        \n",
    "    def Forward(self, X):\n",
    "        mem = torch.zeros(1,self.size_mem)\n",
    "        y_hat = []\n",
    "        for i in range(X.shape[0]):\n",
    "            x_in = X[[i],:]\n",
    "            z_in = torch.cat([x_in,mem],dim = 1)\n",
    "            mem = self.mem_layer.Forward(x_in)\n",
    "            y_hat.append(self.out_layer.Forward(men))\n",
    "        return torch.cat(y_hat,dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(38,15,5)\n",
    "\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "opt = torch.optim.Adam(rnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.1068,  0.3737, -1.1810, -0.3015,  0.5125]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(rnn.mem_layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x38 and 43x5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-91dad97a4b52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-c86f070e76ed>\u001b[0m in \u001b[0;36mForward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mx_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mz_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b906a138340c>\u001b[0m in \u001b[0;36mForward\u001b[0;34m(self, z_in)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x38 and 43x5)"
     ]
    }
   ],
   "source": [
    "y_hat = rnn.Forward(x_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_func(y_hat,y)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "print()\n",
    "print(rnn.mem_layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LSTM Class\n",
    "- forget gate\n",
    "- memory gate\n",
    "- recall gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, size_in,size_out,size_long,size_short):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.size_long = size_long\n",
    "        self.size_short = size_short\n",
    "        \n",
    "        size_z = size_in + size_short\n",
    "        \n",
    "        self.forget_gate = Layer(size_z, size_long, torch.sigmoid)\n",
    "        self.memory_gate = Layer(size_z, size_long, torch.sigmoid)\n",
    "        \n",
    "        self.memory_layer = Layer(size_z, size_long, torch.tanh)\n",
    "        \n",
    "        self.recall_gate = Layer(size_z, size_short, torch.sigmoid)\n",
    "        self.recall_layer = Layer(size_long, size_short, torch.tanh)\n",
    "        self.output_gate = Layer(size_short, size_out, torch.sigmoid)\n",
    "        \n",
    "    def Forward(self, X):\n",
    "        mem_short = torch.zeros(1,self.size_short)\n",
    "        mem_long = torch.zeros(1, self.size_long)\n",
    "        \n",
    "        y_hat = []\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            X_t = X[[i],:]\n",
    "            z_t = torch.cat([X_t,mem_short], dim = 1)\n",
    "            \n",
    "            mem_long = mem_long*self.forget_gate.Forward(z_t)# z_t are the epoch....\n",
    "            mem_long = mem_long + (self.memory_gate.Forward(z_t)*self.memory_layer.Forward(z_t))\n",
    "            mem_short = self.recall_gate.Forward(z_t) + self.recall_layer.Forward(mem_long)\n",
    "            \n",
    "            y_hat.append(self.output_gate.Forward(mem_short))\n",
    "        return torch.cat(y_hat, dim = 0)\n",
    "            \n",
    "            #out = self.output_gate.Forward(mem_short)\n",
    "            #out = torch.argmax(out,dim = 1)\n",
    "    \n",
    "    def Generate(self, start, stop, random_factor):\n",
    "        y_hat = [start]\n",
    "        \n",
    "        mem_long = torch.randn([1,self.size_long])*random_factor\n",
    "        \n",
    "        mem_short = torch.randn([1,self.size_short])* random_factor\n",
    "        \n",
    "        while(y_hat[-1] != stop).any() and len(y_hat) < 30:\n",
    "            x_t = y_hat[-1]\n",
    "            z_t = torch.cat([x_t, mem_short], dim=1)\n",
    "            mem_long = mem_long*self.forget_gate.Forward(z_t)\n",
    "            mem_long = mem_long +(self.memory_gate.Forward(z_t)*self.memory_layer.Forward(z_t))\n",
    "            mem_short = self.recall_gate.Forward(z_t)+self.recall_layer.Forward(mem_long)\n",
    "            out = self.output_gate.Forward(mem_short)\n",
    "            out = torch.argmax(out, dim =1)\n",
    "            \n",
    "            \n",
    "            #OSTM\n",
    "            \n",
    "            y_hat.append(torch.zeros(stop.shape))\n",
    "            y_hat[-1][0,out] = 1\n",
    "        return torch.cat(y_hat, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
