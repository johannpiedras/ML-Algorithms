{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(torch.nn.Module):\n",
    "    def __init__(self, size_in, size_out, activation):\n",
    "            super(Layer, self).__init__()\n",
    "            self.weights = torch.nn.Parameter(torch.randn(size_in, size_out, requires_grad= True))\n",
    "            self.bias = torch.nn.Parameter(\n",
    "            torch.randn(1, size_out, requires_grad= True))\n",
    "            self.activation = activation\n",
    "    def Forward(self, z_in):\n",
    "        return self.activation(z_in @self.weights + self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "forget = Layer(38, 15, torch.nn.Sigmoid())\n",
    "loss_func = torch.nn.MSELoss()\n",
    "opt = torch.optim.Adam(forget.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = torch.randn(1, 38)\n",
    "y = torch.rand(1, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.3688, -1.8626, -1.4306,  0.7421,  0.7392, -1.1659, -0.6857,  0.1837,\n",
      "         -0.2218, -0.4567,  0.0188,  2.0057, -0.8188, -0.9774, -0.8473]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.3678, -1.8616, -1.4296,  0.7431,  0.7402, -1.1649, -0.6847,  0.1847,\n",
      "         -0.2208, -0.4557,  0.0198,  2.0067, -0.8178, -0.9784, -0.8463]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(forget.bias)\n",
    "out = forget.Forward(x_in)\n",
    "loss = loss_func(out, y)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "print(forget.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, size_in, size_out, size_mem):\n",
    "        super(RNN, self).__init__()\n",
    "        self.size_mem = size_mem\n",
    "        self.mem_layer = Layer(size_in + size_mem, size_mem, torch.tanh)\n",
    "        self.out_layer = Layer(size_mem, size_out, torch.sigmoid)\n",
    "        \n",
    "    \n",
    "    def Forward(self, X):\n",
    "        mem = torch.zeros(1, self.size_mem)\n",
    "        y_hat = []\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            x_in = X[[i], :]\n",
    "            z_in = torch.cat([x_in, mem], dim = 1)\n",
    "            mem = self.mem_layer.Forward(z_in)\n",
    "            y_hat.append(self.out_layer.Forward(mem))\n",
    "        return torch.cat(y_hat, dim = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(38, 15, 5)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "opt = torch.optim.Adam(rnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.5090,  0.4356,  0.6866,  0.9813, -0.8067]], requires_grad=True)\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.5090,  0.4346,  0.6876,  0.9804, -0.8077]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(rnn.mem_layer.bias)\n",
    "y_hat = rnn.Forward(x_in)\n",
    "loss = loss_func(y_hat, y)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "print()\n",
    "print(rnn.mem_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build The LSTM Class\n",
    "LSTM stands for Long Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "  def __init__(self, size_in, size_out, size_long, size_short):\n",
    "    super(LSTM, self).__init__()\n",
    "    self.size_long = size_long\n",
    "    self.size_short = size_short\n",
    "\n",
    "    size_z = size_in + size_short\n",
    "\n",
    "    self.forget_gate = Layer(size_z, size_long, torch.sigmoid)\n",
    "    self.memory_gate = Layer(size_z, size_long, torch.sigmoid)\n",
    "    self.memory_layer = Layer(size_z, size_long, torch.tanh)\n",
    "    self.recall_gate = Layer(size_z, size_short, torch.sigmoid)\n",
    "    self.recall_layer = Layer(size_long, size_short, torch.tanh)\n",
    "    self.output_gate = Layer(size_short, size_out, torch.sigmoid)\n",
    "  \n",
    "  def Forward(self, X):\n",
    "    mem_short = torch.zeros(1, self.size_short)\n",
    "    mem_long = torch.zeros(1, self.size_long)\n",
    "    y_hat = []\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "      X_t = X[[i], :]\n",
    "      Z_t = torch.cat([X_t, mem_short], dim = 1)\n",
    "\n",
    "      mem_long = mem_long*self.forget_gate.Forward(Z_t)\n",
    "      mem_long = mem_long + (self.memory_gate.Forward(Z_t)*self.memory_layer.Forward(Z_t))\n",
    "      mem_short = self.recall_gate.Forward(Z_t) * self.recall_layer.Forward(mem_long)\n",
    "\n",
    "      #out = self.output_gate.Forward(mem_short)\n",
    "      #out = torch.argmax(out, dim = 1)\n",
    "\n",
    "      y_hat.append(self.output_gate.Forward(mem_short))\n",
    "\n",
    "    return torch.cat(y_hat, dim = 0)\n",
    "\n",
    "  def Generate(self, start, stop, random_factor):\n",
    "    y_hat = [start]\n",
    "\n",
    "    mem_long = torch.randn([1, self.size_long])*random_factor\n",
    "    mem_short = torch.randn([1, self.size_short])*random_factor\n",
    "\n",
    "    while ((y_hat[-1] != stop).any() and len(y_hat) <30):\n",
    "      X_t = y_hat[-1]\n",
    "      Z_t = torch.cat([X_t, mem_short], dim = 1)\n",
    "      mem_long = mem_long*self.forget_gate.Forward(Z_t)\n",
    "      mem_long = mem_long + (self.memory_gate.Forward(Z_t)*self.memory_layer.Forward(Z_t))\n",
    "      mem_short = self.recall_gate.Forward(Z_t) * self.recall_layer.Forward(mem_long)\n",
    "      out = self.output_gate.Forward(mem_short)\n",
    "      out = torch.argmax(out, dim = 1)\n",
    "      y_hat.append(torch.zeros(stop.shape))\n",
    "      y_hat[-1][0, out] = 1\n",
    "    return torch.cat(y_hat, dim = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aachenosaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardonyx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abelisaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abrictosaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abrosaurus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0  aachenosaurus\n",
       "1       aardonyx\n",
       "2    abelisaurus\n",
       "3  abrictosaurus\n",
       "4     abrosaurus"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"dinosaurs.csv\", sep = \",\", header= None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process(name):\n",
    "  name = ''.join(['{', name,'|'])\n",
    "  out = []\n",
    "\n",
    "  for letter in name:\n",
    "    row = torch.zeros([1, 28])\n",
    "    row[0, ord(letter) - 97] = 1\n",
    "    out.append(row)\n",
    "  return torch.cat(out)\n",
    "\n",
    "def Decode(y_hat):\n",
    "  out = ''\n",
    "  for i in torch.argmax(y_hat, dim = 1):\n",
    "    out += chr(i + 97)\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(28, 28, 12, 12)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.AdamW(lstm.parameters(), lr = 4.5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r\\ Iterations: 0 Loss: -0.8718213438987732 |hccuph|||h \n",
      "r\\ Iterations: 1 Loss: 2.424886465072632 |hccccccccuu|cucc \n",
      "r\\ Iterations: 2 Loss: 2.306680202484131 |nc|ccccccccccce \n",
      "r\\ Iterations: 3 Loss: 2.3542368412017822 |ncccccccccccc \n",
      "r\\ Iterations: 4 Loss: 2.1284780502319336 |nccccccccccc \n",
      "r\\ Iterations: 5 Loss: 2.5068132877349854 |ncccccccccccccc \n",
      "r\\ Iterations: 6 Loss: 2.4067635536193848 |nccccccccc \n",
      "r\\ Iterations: 7 Loss: 2.093230724334717 |nccccccccccccc \n",
      "r\\ Iterations: 8 Loss: 2.1288399696350098 |nccccccccccccccc \n",
      "r\\ Iterations: 9 Loss: 2.4167582988739014 |ncccccccccccccc \n",
      "r\\ Iterations: 10 Loss: 2.14593505859375 |ncccccccucccccccc \n",
      "r\\ Iterations: 11 Loss: 2.171168088912964 |nccccccccccccu \n",
      "r\\ Iterations: 12 Loss: 2.328230619430542 |ncccuucucuccu \n",
      "r\\ Iterations: 13 Loss: 1.9252221584320068 |nccuuucuuuuucucuu \n",
      "r\\ Iterations: 14 Loss: 2.129093885421753 |nccuucuuucucuu \n",
      "r\\ Iterations: 15 Loss: 2.0932202339172363 |n|cuuuu|cucuu \n",
      "r\\ Iterations: 16 Loss: 1.8600008487701416 |nc||uuuucucuu \n",
      "r\\ Iterations: 17 Loss: 1.862502098083496 |nccuuuuuu|uc|u \n",
      "r\\ Iterations: 18 Loss: 2.157046318054199 |nc||uuuu||uuuu \n",
      "r\\ Iterations: 19 Loss: 1.9311487674713135 |n|||c||||u|u| \n",
      "r\\ Iterations: 20 Loss: 1.9726409912109375 |n||||||||||| \n",
      "r\\ Iterations: 21 Loss: 2.125515937805176 |nc|||||||| \n",
      "r\\ Iterations: 22 Loss: 2.2101662158966064 |nc||||||||||| \n",
      "r\\ Iterations: 23 Loss: 2.2195491790771484 |na|||r||||| \n",
      "r\\ Iterations: 24 Loss: 2.060563564300537 |na||||||r||| \n",
      "r\\ Iterations: 25 Loss: 2.345019578933716 |naa||||||rr|r|rr \n",
      "r\\ Iterations: 26 Loss: 1.96529221534729 |na|||rrr|r|rr \n",
      "r\\ Iterations: 27 Loss: 2.00479793548584 |nacrrrrrrrrrrrrr \n",
      "r\\ Iterations: 28 Loss: 1.924675464630127 |nc|rrrrrrrc \n",
      "r\\ Iterations: 29 Loss: 2.243633270263672 |nu|rr|rrrrca \n",
      "r\\ Iterations: 30 Loss: 2.2207603454589844 |ncr|rrarrrrrrrrrrrr \n",
      "r\\ Iterations: 31 Loss: 2.008695602416992 |nc|rrcrrrrrrrrr \n",
      "r\\ Iterations: 32 Loss: 2.0201709270477295 |n|rrrrrrrrrrrrrrr \n",
      "r\\ Iterations: 33 Loss: 1.9917278289794922 |na|arrrrrrca \n",
      "r\\ Iterations: 34 Loss: 2.251598358154297 |narrrrrrrrarrrrrr \n",
      "r\\ Iterations: 35 Loss: 2.0604372024536133 |n|rrrrrrrrara \n",
      "r\\ Iterations: 36 Loss: 1.9820709228515625 |narrrrararrrrrrara \n",
      "r\\ Iterations: 37 Loss: 2.0580391883850098 |naaaarrrrrrarara \n",
      "r\\ Iterations: 38 Loss: 1.9126081466674805 |naaraararrarara \n",
      "r\\ Iterations: 39 Loss: 2.01224684715271 |n|crrrraarrrrara \n",
      "r\\ Iterations: 40 Loss: 2.0402297973632812 |naaaraaarrarara \n",
      "r\\ Iterations: 41 Loss: 2.002859354019165 |naarrrrrrra \n",
      "r\\ Iterations: 42 Loss: 2.0830881595611572 |naaararrarara \n",
      "r\\ Iterations: 43 Loss: 1.9424171447753906 |naaaaarrrrarara \n",
      "r\\ Iterations: 44 Loss: 1.9753665924072266 |ncararrrrrrarara \n",
      "r\\ Iterations: 45 Loss: 1.887749433517456 |naaaaaarrrarara \n",
      "r\\ Iterations: 46 Loss: 1.9002492427825928 |ncaaarrrarara \n",
      "r\\ Iterations: 47 Loss: 2.009535074234009 |naaaaaarrrarara \n",
      "r\\ Iterations: 48 Loss: 1.9893743991851807 |naarrarrrarara \n",
      "r\\ Iterations: 49 Loss: 1.8381330966949463 |aaaarrrrrarura \n",
      "r\\ Iterations: 50 Loss: 1.8888070583343506 |aaaarrrarra \n",
      "r\\ Iterations: 51 Loss: 2.1429624557495117 |aaarauraurrrrura \n",
      "r\\ Iterations: 52 Loss: 2.0641088485717773 |aaaururuurrrrrura \n",
      "r\\ Iterations: 53 Loss: 2.065014362335205 |aauraurrrrrura \n",
      "r\\ Iterations: 54 Loss: 1.9380431175231934 |aaurrrurrrra \n",
      "r\\ Iterations: 55 Loss: 2.1238250732421875 |aaururrrrurura \n",
      "r\\ Iterations: 56 Loss: 1.9657628536224365 |aaaururrrrurura \n",
      "r\\ Iterations: 57 Loss: 2.0594732761383057 |aauurrrrrrurura \n",
      "r\\ Iterations: 58 Loss: 2.109504222869873 |auuruurrarra \n",
      "r\\ Iterations: 59 Loss: 2.0942800045013428 |aaurrrrurra \n",
      "r\\ Iterations: 60 Loss: 1.8804495334625244 |aurrruurrrurura \n",
      "r\\ Iterations: 61 Loss: 2.026160478591919 |aauurrrurruruura \n",
      "r\\ Iterations: 62 Loss: 2.180558204650879 |auurrurrrrurura \n",
      "r\\ Iterations: 63 Loss: 2.0570874214172363 |auurrrururra \n",
      "r\\ Iterations: 64 Loss: 2.041146993637085 |aurruurrrruru \n",
      "r\\ Iterations: 65 Loss: 2.055591344833374 |aauurrrrurura \n",
      "r\\ Iterations: 66 Loss: 2.055356740951538 |aaurrrrurrururu \n",
      "r\\ Iterations: 67 Loss: 1.9357807636260986 |aauururrrru \n",
      "r\\ Iterations: 68 Loss: 1.8814563751220703 |aaruruuaura \n",
      "r\\ Iterations: 69 Loss: 2.1184630393981934 |aaurrrrururu \n",
      "r\\ Iterations: 70 Loss: 2.000807046890259 |aiuurruuuru \n",
      "r\\ Iterations: 71 Loss: 2.065347671508789 |auurururrururu \n",
      "r\\ Iterations: 72 Loss: 1.9314851760864258 |aaururrururu \n",
      "r\\ Iterations: 73 Loss: 1.90736985206604 |aauruuuuuuuruuu \n",
      "r\\ Iterations: 74 Loss: 2.2040295600891113 |aaurururururuuru \n",
      "r\\ Iterations: 75 Loss: 2.027575731277466 |auuuuruururu \n",
      "r\\ Iterations: 76 Loss: 1.9110641479492188 |aouuuuurrrru \n",
      "r\\ Iterations: 77 Loss: 2.1014275550842285 |aouuurruu \n",
      "r\\ Iterations: 78 Loss: 2.046431303024292 |aaurururuuuru \n",
      "r\\ Iterations: 79 Loss: 1.9751780033111572 |aauruururuururu \n",
      "r\\ Iterations: 80 Loss: 1.8937599658966064 |aaurururuuuru \n",
      "r\\ Iterations: 81 Loss: 1.9631092548370361 |aauurruururu \n",
      "r\\ Iterations: 82 Loss: 1.9030206203460693 |aaurruuuuruuru \n",
      "r\\ Iterations: 83 Loss: 1.9945988655090332 |auuuurruururu \n",
      "r\\ Iterations: 84 Loss: 2.1025068759918213 |aouuuuurruururu \n",
      "r\\ Iterations: 85 Loss: 1.873542308807373 |auuuururuuu \n",
      "r\\ Iterations: 86 Loss: 2.1212291717529297 |aauuuuuruuruururu \n",
      "r\\ Iterations: 87 Loss: 2.0997567176818848 |aauurruururu \n",
      "r\\ Iterations: 88 Loss: 1.8926928043365479 |aauururuururu \n",
      "r\\ Iterations: 89 Loss: 1.7907054424285889 |aauruuuruuruuru \n",
      "r\\ Iterations: 90 Loss: 1.9824416637420654 |auuuuuuuru \n",
      "r\\ Iterations: 91 Loss: 2.039931297302246 |aauurrururuururu \n",
      "r\\ Iterations: 92 Loss: 1.9419522285461426 |aauuururuuru \n",
      "r\\ Iterations: 93 Loss: 2.018826961517334 |huuurrurruuu \n",
      "r\\ Iterations: 94 Loss: 2.1128358840942383 |haururruuuuru \n",
      "r\\ Iterations: 95 Loss: 2.1244728565216064 |houururrruruuuruuru \n",
      "r\\ Iterations: 96 Loss: 1.9504997730255127 |harurrrruruururu \n",
      "r\\ Iterations: 97 Loss: 1.9702823162078857 |hauuuuruuuru \n",
      "r\\ Iterations: 98 Loss: 2.0536811351776123 |hourrrrurrururu \n",
      "r\\ Iterations: 99 Loss: 1.9960315227508545 |hauurruururrruru \n",
      "r\\ Iterations: 100 Loss: 2.094787836074829 |harrruurrruuru \n",
      "r\\ Iterations: 101 Loss: 1.974487066268921 |haururrrrururu \n",
      "r\\ Iterations: 102 Loss: 1.8146092891693115 |harrrrrurrra \n",
      "r\\ Iterations: 103 Loss: 1.9019420146942139 |haurrrrrrruru \n",
      "r\\ Iterations: 104 Loss: 1.8718807697296143 |haurrruurrrr \n",
      "r\\ Iterations: 105 Loss: 2.076004981994629 |harurrrrrrrrurr \n",
      "r\\ Iterations: 106 Loss: 2.001539707183838 |haurrurrrrrra \n",
      "r\\ Iterations: 107 Loss: 2.122594118118286 |aarrrrrrrrrurr \n",
      "r\\ Iterations: 108 Loss: 1.959350347518921 |haurrrrrrrrurr \n",
      "r\\ Iterations: 109 Loss: 1.950202465057373 |harrrrrrrrrrurr \n",
      "r\\ Iterations: 110 Loss: 1.9807908535003662 |haurrrrrrrra \n",
      "r\\ Iterations: 111 Loss: 2.1485211849212646 |harurrurrrrrrr \n",
      "r\\ Iterations: 112 Loss: 2.190437078475952 |haurrrrrrurrrr \n",
      "r\\ Iterations: 113 Loss: 2.1031620502471924 |harrrrrrrrrr \n",
      "r\\ Iterations: 114 Loss: 2.035501003265381 |harrrarrrrrrrrrr \n",
      "r\\ Iterations: 115 Loss: 2.1148927211761475 |harrrrrrrruru \n",
      "r\\ Iterations: 116 Loss: 1.8188750743865967 |harrrrrrrrrrru \n",
      "r\\ Iterations: 117 Loss: 2.0869219303131104 |harurrrrrrr \n",
      "r\\ Iterations: 118 Loss: 2.1821846961975098 |harrrrrrrrrrrrr \n",
      "r\\ Iterations: 119 Loss: 1.9271538257598877 |aarrrrrrr \n",
      "r\\ Iterations: 120 Loss: 1.862274408340454 |aarrrruururrrra \n",
      "r\\ Iterations: 121 Loss: 2.0387651920318604 |aaurrarrrrruru \n",
      "r\\ Iterations: 122 Loss: 1.8440120220184326 |aaraurrrrrruru \n",
      "r\\ Iterations: 123 Loss: 1.9110567569732666 |aarrrrrarrrrrrra \n",
      "r\\ Iterations: 124 Loss: 2.033543109893799 |aarrrrrrrrruru \n",
      "r\\ Iterations: 125 Loss: 1.909088373184204 |aarrrarrrrruru \n",
      "r\\ Iterations: 126 Loss: 1.9076414108276367 |aarrrarurrrrr \n",
      "r\\ Iterations: 127 Loss: 2.246216058731079 |aararuruurra \n",
      "r\\ Iterations: 128 Loss: 2.0477280616760254 |aaarrrrrururu \n",
      "r\\ Iterations: 129 Loss: 1.9159622192382812 |aauurrrrrrururu \n",
      "r\\ Iterations: 130 Loss: 2.0776302814483643 |aaurrrrrruru \n",
      "r\\ Iterations: 131 Loss: 2.0058348178863525 |aaurrurururuururu \n",
      "r\\ Iterations: 132 Loss: 2.0227949619293213 |aarurrrurrruururu \n",
      "r\\ Iterations: 133 Loss: 2.0526111125946045 |aoourrrrru \n",
      "r\\ Iterations: 134 Loss: 2.1422319412231445 |aaruurrrrurrruururu \n",
      "r\\ Iterations: 135 Loss: 2.075364112854004 |aoourrrrururu \n",
      "r\\ Iterations: 136 Loss: 1.8394622802734375 |aarurrururrru \n",
      "r\\ Iterations: 137 Loss: 1.9581305980682373 |aaarurruuru \n",
      "r\\ Iterations: 138 Loss: 1.8915727138519287 |aarrurrrururu \n",
      "r\\ Iterations: 139 Loss: 1.9824779033660889 |aarrrurrrrrrruru \n",
      "r\\ Iterations: 140 Loss: 2.1371140480041504 |aoarrrrrrruuurrra \n",
      "r\\ Iterations: 141 Loss: 2.0015571117401123 |aorrrrrrrrr \n",
      "r\\ Iterations: 142 Loss: 1.9995393753051758 |aarorrrrururu \n",
      "r\\ Iterations: 143 Loss: 1.913250207901001 |aaarrrrrru \n",
      "r\\ Iterations: 144 Loss: 1.9538600444793701 |aoarurrrururu \n",
      "r\\ Iterations: 145 Loss: 1.826897382736206 |aarrrrrrrorrrrruru \n",
      "r\\ Iterations: 146 Loss: 2.068836212158203 |aaorrorrrrruru \n",
      "r\\ Iterations: 147 Loss: 2.1025187969207764 |aaarrourorururu \n",
      "r\\ Iterations: 148 Loss: 1.9139716625213623 |aaourouruurorururu \n",
      "r\\ Iterations: 149 Loss: 2.016467809677124 |aoaoorrrrrorururu \n",
      "r\\ Iterations: 150 Loss: 1.8357832431793213 |aoaorrrorrru \n",
      "r\\ Iterations: 151 Loss: 2.079967737197876 |aaarrrorarro \n",
      "r\\ Iterations: 152 Loss: 2.117145538330078 |aaorrorrruru \n",
      "r\\ Iterations: 153 Loss: 1.8856737613677979 |aaaoorrrrorrruru \n",
      "r\\ Iterations: 154 Loss: 1.972748041152954 |aaaaooaaororrruou \n",
      "r\\ Iterations: 155 Loss: 1.9496521949768066 |aaoaaourououuou \n",
      "r\\ Iterations: 156 Loss: 1.9820094108581543 |aaaoororrouou \n",
      "r\\ Iterations: 157 Loss: 1.8089869022369385 |aaoaaoaoaaoooouou \n",
      "r\\ Iterations: 158 Loss: 1.9989521503448486 |aaaooaoaaooaaoa \n",
      "r\\ Iterations: 159 Loss: 2.1366190910339355 |aoaaoaoaoaoa \n",
      "r\\ Iterations: 160 Loss: 2.1956946849823 |aaaoaoaoaooa \n",
      "r\\ Iterations: 161 Loss: 2.050055503845215 |aaoaaaaoa \n",
      "r\\ Iterations: 162 Loss: 2.0096588134765625 |aoaaouuroruouuu \n",
      "r\\ Iterations: 163 Loss: 1.8517024517059326 |aaorrrrourouuu \n",
      "r\\ Iterations: 164 Loss: 2.062213897705078 |aaouorrrurururuuu \n",
      "r\\ Iterations: 165 Loss: 1.9211642742156982 |aourruroruroro \n",
      "r\\ Iterations: 166 Loss: 2.1226651668548584 |aauouuururruruu \n",
      "r\\ Iterations: 167 Loss: 2.1764283180236816 |aauuurourruruuu \n",
      "r\\ Iterations: 168 Loss: 2.040198802947998 |aaorrrrrururuuu \n",
      "r\\ Iterations: 169 Loss: 1.8744959831237793 |aaorrururrurru \n",
      "r\\ Iterations: 170 Loss: 2.01582932472229 |aarurrururuuu \n",
      "r\\ Iterations: 171 Loss: 1.9248504638671875 |aaauurrrrurrru \n",
      "r\\ Iterations: 172 Loss: 2.216190814971924 |aarrrurururuuu \n",
      "r\\ Iterations: 173 Loss: 1.8233296871185303 |aaorrurrruuu \n",
      "r\\ Iterations: 174 Loss: 1.893491506576538 |aaouururrrurrruuu \n",
      "r\\ Iterations: 175 Loss: 1.8541574478149414 |aourrururrruuu \n",
      "r\\ Iterations: 176 Loss: 1.9625065326690674 |aauorrrurrrrruu \n",
      "r\\ Iterations: 177 Loss: 1.9294350147247314 |aaurrrrurrrrrrruu \n",
      "r\\ Iterations: 178 Loss: 2.058547019958496 |aaaorrrrrrrrra \n",
      "r\\ Iterations: 179 Loss: 1.969923734664917 |aaurrrrrrra \n",
      "r\\ Iterations: 180 Loss: 1.919245719909668 |aoarrorrrarororuu \n",
      "r\\ Iterations: 181 Loss: 1.9933204650878906 |aarrurrraur \n",
      "r\\ Iterations: 182 Loss: 1.747297763824463 |aoaorrrrra \n",
      "r\\ Iterations: 183 Loss: 2.0873918533325195 |aaaorarrrrrara \n",
      "r\\ Iterations: 184 Loss: 1.9551723003387451 |aarorrarrrrrarr \n",
      "r\\ Iterations: 185 Loss: 1.9187633991241455 |aaaurrrrrrrarr \n",
      "r\\ Iterations: 186 Loss: 1.7626545429229736 |aaarrrrrrrr \n",
      "r\\ Iterations: 187 Loss: 2.0738911628723145 |aarrrrrrrrrrrrr \n",
      "r\\ Iterations: 188 Loss: 1.8805081844329834 |aoaarrrrrrrr \n",
      "r\\ Iterations: 189 Loss: 1.8333516120910645 |aarrrrrrrra \n",
      "r\\ Iterations: 190 Loss: 2.1229870319366455 |aarrurrrarra \n",
      "r\\ Iterations: 191 Loss: 2.183973550796509 |aarrrurrua \n",
      "r\\ Iterations: 192 Loss: 2.2424476146698 |aaaurrrru \n",
      "r\\ Iterations: 193 Loss: 2.1759424209594727 |aaruururrrrrru \n",
      "r\\ Iterations: 194 Loss: 1.9515912532806396 |aaorrururrrruu \n",
      "r\\ Iterations: 195 Loss: 1.82490873336792 |aauuruurrruu \n",
      "r\\ Iterations: 196 Loss: 1.9462156295776367 |aarrrrrurrrrru \n",
      "r\\ Iterations: 197 Loss: 2.0159800052642822 |aaorruuurrrruuu \n",
      "r\\ Iterations: 198 Loss: 2.025270462036133 |aorrurrurrrruu \n",
      "r\\ Iterations: 199 Loss: 1.919753074645996 |aauuururrrruu \n",
      "r\\ Iterations: 200 Loss: 1.9772987365722656 |aoarrrurrrruu \n",
      "r\\ Iterations: 201 Loss: 1.79134202003479 |aoouuuuuurrrruu \n",
      "r\\ Iterations: 202 Loss: 2.0847225189208984 |aaurruuurrrruu \n",
      "r\\ Iterations: 203 Loss: 1.9491238594055176 |aaouurrrurrrruu \n",
      "r\\ Iterations: 204 Loss: 1.773552417755127 |aaorrrurrrruu \n",
      "r\\ Iterations: 205 Loss: 1.9397609233856201 |aoruuruuruuu \n",
      "r\\ Iterations: 206 Loss: 2.097898006439209 |aaoruurrruuuru \n",
      "r\\ Iterations: 207 Loss: 2.091245174407959 |aaururrrurruu \n",
      "r\\ Iterations: 208 Loss: 2.14494252204895 |aouuuuuruuururuuu \n",
      "r\\ Iterations: 209 Loss: 1.9797062873840332 |aaruurrururuuuuu \n",
      "r\\ Iterations: 210 Loss: 1.8803770542144775 |aauuuuuuurrruu \n",
      "r\\ Iterations: 211 Loss: 2.2614195346832275 |aaroruurururru \n",
      "r\\ Iterations: 212 Loss: 1.9608888626098633 |aarurruruouuu \n",
      "r\\ Iterations: 213 Loss: 1.849611520767212 |aaruurrruurou \n",
      "r\\ Iterations: 214 Loss: 2.0186080932617188 |aaouorruuu \n",
      "r\\ Iterations: 215 Loss: 1.9974572658538818 |aauuurrururuouuu \n",
      "r\\ Iterations: 216 Loss: 1.8367125988006592 |aaarruruouuu \n",
      "r\\ Iterations: 217 Loss: 1.7630236148834229 |aarrruuruouuu \n",
      "r\\ Iterations: 218 Loss: 1.7562952041625977 |aauuuuuurruuuuuuuu \n",
      "r\\ Iterations: 219 Loss: 2.0325121879577637 |aauouuoruuuu \n",
      "r\\ Iterations: 220 Loss: 1.9752671718597412 |aoarrrrrrruruouuu \n",
      "r\\ Iterations: 221 Loss: 1.8801324367523193 |aaaourruuuuuuurru \n",
      "r\\ Iterations: 222 Loss: 2.0724399089813232 |aaruuouurrouuu \n",
      "r\\ Iterations: 223 Loss: 1.9343047142028809 |aauououurrouuu \n",
      "r\\ Iterations: 224 Loss: 1.890669822692871 |aoarrruurruruuuu \n",
      "r\\ Iterations: 225 Loss: 1.9323949813842773 |aaaurroaarurroauu \n",
      "r\\ Iterations: 226 Loss: 1.8218646049499512 |aaaruurrrrurroauu \n",
      "r\\ Iterations: 227 Loss: 1.9042534828186035 |aaruuoaoaoauu \n",
      "r\\ Iterations: 228 Loss: 1.947190284729004 |aarrrrruurraraauu \n",
      "r\\ Iterations: 229 Loss: 2.1022934913635254 |aaoroarurrorraua \n",
      "r\\ Iterations: 230 Loss: 1.8736634254455566 |aarrrrrurroaoa \n",
      "r\\ Iterations: 231 Loss: 1.8796441555023193 |aaaoarurroaoa \n",
      "r\\ Iterations: 232 Loss: 1.8458821773529053 |aaoaoraruraoaaaaua \n",
      "r\\ Iterations: 233 Loss: 1.9254322052001953 |aasoraauraoaua \n",
      "r\\ Iterations: 234 Loss: 1.925461769104004 |aoaoaurroaua \n",
      "r\\ Iterations: 235 Loss: 1.8028924465179443 |aasrarraaaauraoaua \n",
      "r\\ Iterations: 236 Loss: 2.0320279598236084 |aasrarososua \n",
      "r\\ Iterations: 237 Loss: 2.1266555786132812 |aasoaarasra \n",
      "r\\ Iterations: 238 Loss: 2.0324230194091797 |aasrrasorra \n",
      "r\\ Iterations: 239 Loss: 1.9373159408569336 |aasrarrra \n",
      "r\\ Iterations: 240 Loss: 2.176912546157837 |aaurrrrra \n",
      "r\\ Iterations: 241 Loss: 2.2660889625549316 |aaorurorurroaur \n",
      "r\\ Iterations: 242 Loss: 1.7618136405944824 |aaaarrrarra \n",
      "r\\ Iterations: 243 Loss: 1.9847536087036133 |paouururrrurrruuu \n",
      "r\\ Iterations: 244 Loss: 1.8406829833984375 |porrrrrrrrr \n",
      "r\\ Iterations: 245 Loss: 2.0378847122192383 |paorrrrrurrruuu \n",
      "r\\ Iterations: 246 Loss: 1.9044697284698486 |paaurrurrr \n",
      "r\\ Iterations: 247 Loss: 2.108008861541748 |parrurrrururrruuu \n",
      "r\\ Iterations: 248 Loss: 1.9019379615783691 |parrurrrururrr \n",
      "r\\ Iterations: 249 Loss: 1.9988329410552979 |pauururruuuu \n",
      "r\\ Iterations: 250 Loss: 1.9410057067871094 |paruurrururruuuu \n",
      "r\\ Iterations: 251 Loss: 1.8103070259094238 |parurururruuuu \n",
      "r\\ Iterations: 252 Loss: 1.9485180377960205 |paruururruuuu \n",
      "r\\ Iterations: 253 Loss: 1.9240858554840088 |pauruururruuuu \n",
      "r\\ Iterations: 254 Loss: 1.8945176601409912 |pauuuururruuuu \n",
      "r\\ Iterations: 255 Loss: 1.8339028358459473 |paauuurauuurrruu \n",
      "r\\ Iterations: 256 Loss: 2.06170916557312 |paauuuurruururuuu \n",
      "r\\ Iterations: 257 Loss: 2.055295705795288 |paaurrururru \n",
      "r\\ Iterations: 258 Loss: 1.7752344608306885 |paruururruuuu \n",
      "r\\ Iterations: 259 Loss: 1.8555164337158203 |parrurrrruuu \n",
      "r\\ Iterations: 260 Loss: 2.0810112953186035 |parrrrrurruuuu \n",
      "r\\ Iterations: 261 Loss: 1.943221092224121 |parrurrururruuuu \n",
      "r\\ Iterations: 262 Loss: 1.8535583019256592 |pauuruuuuruu \n",
      "r\\ Iterations: 263 Loss: 2.0306220054626465 |paaurrrurruuuu \n",
      "r\\ Iterations: 264 Loss: 1.9801201820373535 |parrrurruuuu \n",
      "r\\ Iterations: 265 Loss: 1.8758044242858887 |paaurrrurruuuu \n",
      "r\\ Iterations: 266 Loss: 1.933410406112671 |pauruuruuruuuuu \n",
      "r\\ Iterations: 267 Loss: 1.860962152481079 |paauuruuuuuuu \n",
      "r\\ Iterations: 268 Loss: 2.00858473777771 |auruuuuuru \n",
      "r\\ Iterations: 269 Loss: 2.2031168937683105 |aauuuuuuuuuuuu \n",
      "r\\ Iterations: 270 Loss: 2.1300082206726074 |auuuuruuuuuuu \n",
      "r\\ Iterations: 271 Loss: 1.9347407817840576 |aauuuuuuuuuuu \n",
      "r\\ Iterations: 272 Loss: 1.8602476119995117 |auruuuuuuuuuuu \n",
      "r\\ Iterations: 273 Loss: 2.0416042804718018 |auuuuuuuuuuu \n",
      "r\\ Iterations: 274 Loss: 1.7769320011138916 |auuuuuuuuuuuuuuuuuu \n",
      "r\\ Iterations: 275 Loss: 2.1181459426879883 |auuuuuuuuuuuu \n",
      "r\\ Iterations: 276 Loss: 1.813079833984375 |auuuuuuuuuuuuuu \n",
      "r\\ Iterations: 277 Loss: 1.8587794303894043 |auuuuuuuuuuuuu \n",
      "r\\ Iterations: 278 Loss: 2.0741090774536133 |auuuuuuuuu \n",
      "r\\ Iterations: 279 Loss: 2.155561685562134 |auuuuuuuuuuu \n",
      "r\\ Iterations: 280 Loss: 2.0339200496673584 |auuuuuuuuuuuuuuuu \n",
      "r\\ Iterations: 281 Loss: 1.8192026615142822 |auuuuuuuu \n",
      "r\\ Iterations: 282 Loss: 1.8809010982513428 |auuuuuuuuuuuuuu \n",
      "r\\ Iterations: 283 Loss: 1.9313335418701172 |auuuuuuuuuuuuuuu \n",
      "r\\ Iterations: 284 Loss: 1.9143011569976807 |auuuuuuuuuuuuuuu \n",
      "r\\ Iterations: 285 Loss: 1.8076684474945068 |auuuuuuuuuuuuuuuu \n",
      "r\\ Iterations: 286 Loss: 1.9274241924285889 |auuuuuuuuuuuuu \n",
      "r\\ Iterations: 287 Loss: 1.8593125343322754 |auuuuuuuuuuuuu \n",
      "r\\ Iterations: 288 Loss: 1.8610737323760986 |auuuuuuuuuu \n",
      "r\\ Iterations: 289 Loss: 1.9020311832427979 |auuuuuuuuu \n",
      "r\\ Iterations: 290 Loss: 1.8155238628387451 |auuuuuuuuuuuu \n",
      "r\\ Iterations: 291 Loss: 2.016695976257324 |auuuuuuuuuuuuu \n",
      "r\\ Iterations: 292 Loss: 1.8465502262115479 |auuuuuuuuuuuuuu \n",
      "r\\ Iterations: 293 Loss: 1.832688570022583 |auuuruuuuuuu \n",
      "r\\ Iterations: 294 Loss: 1.997253656387329 |auururuuruuu \n",
      "r\\ Iterations: 295 Loss: 1.9366273880004883 |auaururuuuuuuuuuuuu \n",
      "r\\ Iterations: 296 Loss: 2.104487419128418 |auururuuruuu \n",
      "r\\ Iterations: 297 Loss: 1.9278464317321777 |auuuurururuuuuu \n",
      "r\\ Iterations: 298 Loss: 1.788536787033081 |auuruuuuuuuuuurru \n",
      "r\\ Iterations: 299 Loss: 2.0964548587799072 |auuuuuuuuruuuuu \n",
      "r\\ Iterations: 300 Loss: 1.7423090934753418 |aurururuuuuu \n",
      "r\\ Iterations: 301 Loss: 2.2115838527679443 |auururruruuuuu \n",
      "r\\ Iterations: 302 Loss: 1.9035999774932861 |auuuuuuuuururuuu \n",
      "r\\ Iterations: 303 Loss: 2.024245023727417 |auruuuruuruuuuru \n",
      "r\\ Iterations: 304 Loss: 2.1135802268981934 |aarurururuuuuu \n",
      "r\\ Iterations: 305 Loss: 1.7901124954223633 |auruurururuuuuu \n",
      "r\\ Iterations: 306 Loss: 1.8558096885681152 |aauururuuururuuuuu \n",
      "r\\ Iterations: 307 Loss: 1.9553718566894531 |hauruuuuuuuuruu \n",
      "r\\ Iterations: 308 Loss: 2.039003849029541 |haauuuuuuruuuuu \n",
      "r\\ Iterations: 309 Loss: 1.8256285190582275 |husuuruuuuuuuuuu \n",
      "r\\ Iterations: 310 Loss: 1.8423945903778076 |hurrruruuuu \n",
      "r\\ Iterations: 311 Loss: 1.9123241901397705 |hauruuururuuuuu \n",
      "r\\ Iterations: 312 Loss: 1.7489590644836426 |hasururuuuruuuuu \n",
      "r\\ Iterations: 313 Loss: 1.8048555850982666 |huuuuruuuruu \n",
      "r\\ Iterations: 314 Loss: 1.7379720211029053 |huusuusuuuuu \n",
      "r\\ Iterations: 315 Loss: 1.9762914180755615 |hauususuruuuuu \n",
      "r\\ Iterations: 316 Loss: 1.8458094596862793 |hauuususuuuuu \n",
      "r\\ Iterations: 317 Loss: 1.9298152923583984 |hauuusururuuuuu \n",
      "r\\ Iterations: 318 Loss: 1.8694729804992676 |hauuuusuruuuuu \n",
      "r\\ Iterations: 319 Loss: 1.7987031936645508 |hausuuuuruuuuu \n",
      "r\\ Iterations: 320 Loss: 1.8471362590789795 |haruruuuruuuuu \n",
      "r\\ Iterations: 321 Loss: 1.8161330223083496 |huruuruururuuru \n",
      "r\\ Iterations: 322 Loss: 1.925048589706421 |haaurusuruuuuu \n",
      "r\\ Iterations: 323 Loss: 1.7647745609283447 |hasruururuaurusuuu \n",
      "r\\ Iterations: 324 Loss: 1.972224235534668 |haausuuuu \n",
      "r\\ Iterations: 325 Loss: 2.1157164573669434 |hurururuuuuu \n",
      "r\\ Iterations: 326 Loss: 1.6899309158325195 |huuuusuruuuuu \n",
      "r\\ Iterations: 327 Loss: 1.8574647903442383 |haaauuruuruuuuu \n",
      "r\\ Iterations: 328 Loss: 1.8449881076812744 |hauruusususua \n",
      "r\\ Iterations: 329 Loss: 1.9858224391937256 |hsraususuruuuuu \n",
      "r\\ Iterations: 330 Loss: 1.774129867553711 |husurusuu \n",
      "r\\ Iterations: 331 Loss: 2.0165963172912598 |haasusuruuuuu \n",
      "r\\ Iterations: 332 Loss: 1.7606449127197266 |husurusuruuuuu \n",
      "r\\ Iterations: 333 Loss: 1.7200312614440918 |harusuururuuuuu \n",
      "r\\ Iterations: 334 Loss: 1.8042614459991455 |huruususuuuu \n",
      "r\\ Iterations: 335 Loss: 2.0165321826934814 |huusurusuruusrusuu \n",
      "r\\ Iterations: 336 Loss: 1.9451160430908203 |hausuuuruusuuruuuuu \n",
      "r\\ Iterations: 337 Loss: 1.8585782051086426 |haruuuuuuuruu \n",
      "r\\ Iterations: 338 Loss: 1.8733904361724854 |hausuruuuruuuuuuuuu \n",
      "r\\ Iterations: 339 Loss: 2.0410139560699463 |haaruuuuusuruuuuu \n",
      "r\\ Iterations: 340 Loss: 1.7964050769805908 |hsuusuuurusuu \n",
      "r\\ Iterations: 341 Loss: 1.9173095226287842 |haususuuruusuruuuuu \n",
      "r\\ Iterations: 342 Loss: 1.8354806900024414 |hasuauruuuuuuu \n",
      "r\\ Iterations: 343 Loss: 1.9294257164001465 |huruusuuuu \n",
      "r\\ Iterations: 344 Loss: 2.129086971282959 |haurauuuruuruuu \n",
      "r\\ Iterations: 345 Loss: 2.0523717403411865 |hausaausuruuuuu \n",
      "r\\ Iterations: 346 Loss: 1.7493717670440674 |haruausuara \n",
      "r\\ Iterations: 347 Loss: 1.8619141578674316 |hurauauuaura \n",
      "r\\ Iterations: 348 Loss: 2.103238582611084 |huuaasurruuuu \n",
      "r\\ Iterations: 349 Loss: 1.8440754413604736 |hururuaurruuuu \n",
      "r\\ Iterations: 350 Loss: 1.7097890377044678 |hasururaua \n",
      "r\\ Iterations: 351 Loss: 2.0366039276123047 |huaururauaua \n",
      "r\\ Iterations: 352 Loss: 1.8820006847381592 |husuraasusurruaua \n",
      "r\\ Iterations: 353 Loss: 1.8231067657470703 |huauauaaurruaua \n",
      "r\\ Iterations: 354 Loss: 1.8713858127593994 |huaurrurruaua \n",
      "r\\ Iterations: 355 Loss: 1.7627074718475342 |huasaraausua \n",
      "r\\ Iterations: 356 Loss: 2.08560848236084 |husrraaaaraaa \n",
      "r\\ Iterations: 357 Loss: 2.098264455795288 |hausaaurusaua \n",
      "r\\ Iterations: 358 Loss: 1.8780193328857422 |hasaaurusaua \n",
      "r\\ Iterations: 359 Loss: 1.8982043266296387 |haruaraauusua \n",
      "r\\ Iterations: 360 Loss: 1.9751462936401367 |hausauauarurrurur \n",
      "r\\ Iterations: 361 Loss: 1.819514513015747 |huururuasurur \n",
      "r\\ Iterations: 362 Loss: 1.8375089168548584 |hauarraasurrurur \n",
      "r\\ Iterations: 363 Loss: 2.0352065563201904 |hususaaaurasurruuur \n",
      "r\\ Iterations: 364 Loss: 1.8947744369506836 |haausuusaraaurusuur \n",
      "r\\ Iterations: 365 Loss: 1.8791141510009766 |huarusuusurruuur \n",
      "r\\ Iterations: 366 Loss: 1.9850082397460938 |hsuussurruuuu \n",
      "r\\ Iterations: 367 Loss: 1.9293994903564453 |huusuurrruurruuuu \n",
      "r\\ Iterations: 368 Loss: 1.916459083557129 |husuusarurruuuu \n",
      "r\\ Iterations: 369 Loss: 1.9038166999816895 |harsuusurruuuu \n",
      "r\\ Iterations: 370 Loss: 1.9333765506744385 |hasaasurusrra \n",
      "r\\ Iterations: 371 Loss: 2.1926472187042236 |husuusuuuu \n",
      "r\\ Iterations: 372 Loss: 1.9111900329589844 |housusarusuu \n",
      "r\\ Iterations: 373 Loss: 1.961974859237671 |hasusurusuruuuuu \n",
      "r\\ Iterations: 374 Loss: 1.759354591369629 |husurusuruu \n",
      "r\\ Iterations: 375 Loss: 1.90232253074646 |hsrsuusuruuuuu \n",
      "r\\ Iterations: 376 Loss: 1.8123762607574463 |hasusurusuuu \n",
      "r\\ Iterations: 377 Loss: 1.8003928661346436 |hauusuuuuuuu \n",
      "r\\ Iterations: 378 Loss: 1.9778833389282227 |haususuruuuuu \n",
      "r\\ Iterations: 379 Loss: 1.8213236331939697 |hausurruusuuuuu \n",
      "r\\ Iterations: 380 Loss: 1.9809107780456543 |hasauuusuruuuuu \n",
      "r\\ Iterations: 381 Loss: 1.879530668258667 |husuususur \n",
      "r\\ Iterations: 382 Loss: 1.9380543231964111 |hesuuussususru \n",
      "r\\ Iterations: 383 Loss: 2.199964761734009 |hsausaruuusuruuuur \n",
      "r\\ Iterations: 384 Loss: 1.8244822025299072 |hasrusususuuuur \n",
      "r\\ Iterations: 385 Loss: 1.9900367259979248 |huusuususur \n",
      "r\\ Iterations: 386 Loss: 2.127981185913086 |hasusususur \n",
      "r\\ Iterations: 387 Loss: 1.8522441387176514 |hasuusarususaur \n",
      "r\\ Iterations: 388 Loss: 1.8591969013214111 |herurusuua \n",
      "r\\ Iterations: 389 Loss: 2.224278688430786 |haraurusur \n",
      "r\\ Iterations: 390 Loss: 1.920910358428955 |huraususuruurur \n",
      "r\\ Iterations: 391 Loss: 1.9163048267364502 |huuururrurur \n",
      "r\\ Iterations: 392 Loss: 1.84909987449646 |huusususur \n",
      "r\\ Iterations: 393 Loss: 2.062408208847046 |hauurasusur \n",
      "r\\ Iterations: 394 Loss: 2.093588352203369 |haususuuruusurrurur \n",
      "r\\ Iterations: 395 Loss: 1.8049535751342773 |hausususuurur \n",
      "r\\ Iterations: 396 Loss: 1.8228223323822021 |haususuuruusurrurur \n",
      "r\\ Iterations: 397 Loss: 1.8036789894104004 |hrusuuruususur \n",
      "r\\ Iterations: 398 Loss: 1.987252950668335 |hasussurrurur \n",
      "r\\ Iterations: 399 Loss: 1.7859859466552734 |haurrususur \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff898a93c90>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = 400\n",
    "losses = [0]\n",
    "name = '{'\n",
    "\n",
    "for i in range(iterations):    \n",
    "    name = Process(np.random.choice(data.name))\n",
    "    y_hat = lstm.Forward(name)\n",
    "    print('r\\ Iterations: {} Loss: {} |{}'.format(i, loss - 1, Decode(y_hat)) + ' ')\n",
    "    loss = loss_func(y_hat[:-1], torch.argmax(name,dim=1)[1:])\n",
    "    \n",
    "    loss.backward()\n",
    "    losses.append(loss.detach())\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3e95ac5cb27a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{'\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(Decode(lstm.Generate(Process('{'[[1]]), Process('')[[1]],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MoW7xP6NeNDr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LSTMs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
