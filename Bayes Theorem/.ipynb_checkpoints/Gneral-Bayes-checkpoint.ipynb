{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from scipy.stats import multinomial as mlvn\n",
    "from scipy.stats import bernoulli as brn\n",
    "# A multivariate normal random variable. Think about a 3 dimensionsal guassian \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenBayes():\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, DistStr, epsilon = 1e-3):\n",
    "        \n",
    "        self.likelihoods = dict() #\n",
    "        self.priors = dict() #\n",
    "        \n",
    "        self.K = set(y.astype(int)) #\n",
    "        \n",
    "        \n",
    "        if DistStr == \"Gauss\":\n",
    "            \n",
    "            for k in self.K:\n",
    "                X_k = X[y == k,:] # All the values in class k\n",
    "                N_k, D = X_k.shape # N_k number of observations, D is number of features\n",
    "                mu_k = X_k.mean(axis=0)\n",
    "                \n",
    "                self.likelihoods[k] = {'mean': X_k.mean(axis=0) , 'cov' : (1/(N_k -1)) \n",
    "                                       * np.matmul((X_k - mu_k).T, X_k - mu_k)\n",
    "                                      + epsilon * np.identity(D)}\n",
    "                \n",
    "                self.priors[k] = len(X_k)/len(X)\n",
    "                # we dont return anything because we are using a global variable \n",
    "                \n",
    "                \n",
    "        if DistStr == \"Multinomial\":\n",
    "            for k in self.K:\n",
    "                X_k = X[y == k,:] # All the values in class k\n",
    "                N_k, D = X_k.shape # N_k number of observations, D is number of features\n",
    "                mu_k = X_k.mean(axis=0)\n",
    "                \n",
    "                N = len(X)\n",
    "                self.likihoods[k] = {\"N\" : N, \"p\" : sum(N_k/len(X))}\n",
    "                self.priors[k] = len(X_k)/len(X)\n",
    "                \n",
    "                \n",
    "        if DistStr == \"Bernoulli\":\n",
    "            for k in self.K:\n",
    "                X_k = X[y == k, :]\n",
    "                N_k, D = X_k.shape\n",
    "                \n",
    "                self.likelihoods[k] = {\"P\" : N_k/len(x)}\n",
    "                self.priors[k] = len(X_k)/len(X)\n",
    "    \n",
    "    def predict(self, X, DistStr):\n",
    "        \n",
    "        N, D = X.shape\n",
    "        \n",
    "        if DistStr == \"Gauss\":\n",
    "            P_hat = np.zeros((N, len(self.K)))\n",
    "            \n",
    "            for k, l in self.likelihoods.items():\n",
    "                P_hat[:,k] = mvn.logpdf(X, l['mean'],l['cov']) + np.log(self.priors[k])\n",
    "        \n",
    "            return P_hat.argmax(axis = 1)\n",
    "        \n",
    "        \n",
    "        if DistStr == \"Multinomial\":\n",
    "            P_hat = np.zeros((N, len(self.K)))\n",
    "            \n",
    "            for k, l in self.likelihoods.items():\n",
    "                P_hat[:,k] = mlvn.logpdf(X, l['mean'],l['cov']) + np.log(self.priors[k])\n",
    "        \n",
    "            return P_hat.argmax(axis = 1)\n",
    "                \n",
    "                \n",
    "        if DistStr == \"Bernoulli\":\n",
    "            P_hat = np.zeros((N, len(self.K)))\n",
    "            \n",
    "            for k, l in self.likelihoods.items():\n",
    "                P_hat[:,k] = brn.logpdf(X, l['mean'],l['cov']) + np.log(self.priors[k])\n",
    "            return P_hat.argmax(axis = 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definition \n",
    "def accuracy(y,y_hat):\n",
    "    return np.mean(y == y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('xor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.to_numpy()\n",
    "y = X[:,-1] # storing just the labels\n",
    "X = X[:,:-1] # Dropping the the last column, AKA the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GenBayes()\n",
    "# gm = GenBayes()\n",
    "# gb = GenBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DisStr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-90fdf59640c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Gauss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-f727618dc0f3>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, DistStr, epsilon)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mDisStr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Multinomial\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mX_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# All the values in class k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DisStr' is not defined"
     ]
    }
   ],
   "source": [
    "g.fit(X, y,'Gauss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = g.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
