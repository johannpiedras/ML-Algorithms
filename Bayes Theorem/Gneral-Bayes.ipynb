{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from scipy.stats import multinomial as mlvn\n",
    "from scipy.stats import bernoulli as brn\n",
    "# A multivariate normal random variable. Think about a 3 dimensionsal guassian \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenBayes():\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, DistStr, epsilon = 1e-3):\n",
    "        \n",
    "        self.likelihoods = dict() #\n",
    "        self.priors = dict() #\n",
    "        \n",
    "        self.K = set(y.astype(int)) #\n",
    "        \n",
    "        \n",
    "        if DistStr == \"Gauss\":\n",
    "            \n",
    "            for k in self.K:\n",
    "                X_k = X[y == k,:] # All the values in class k\n",
    "                N_k, D = X_k.shape # N_k number of observations, D is number of features\n",
    "                mu_k = X_k.mean(axis=0)\n",
    "                \n",
    "                self.likelihoods[k] = {'mean': X_k.mean(axis=0) , 'cov' : (1/(N_k -1)) \n",
    "                                       * np.matmul((X_k - mu_k).T, X_k - mu_k)\n",
    "                                      + epsilon * np.identity(D)}\n",
    "                \n",
    "                self.priors[k] = len(X_k)/len(X)\n",
    "                # we dont return anything because we are using a global variable \n",
    "                \n",
    "                \n",
    "        if DistStr == \"Multinomial\":\n",
    "            for k in self.K:\n",
    "                X_k = X[y == k,:] # All the values in class k\n",
    "                N_k, D = X_k.shape # N_k number of observations, D is number of features\n",
    "                mu_k = X_k.mean(axis=0)\n",
    "                \n",
    "                N = len(X)\n",
    "                self.likihoods[k] = {\"N\" : N, \"p\" : sum(N_k/len(X))}\n",
    "                self.priors[k] = len(X_k)/len(X)\n",
    "                \n",
    "                \n",
    "        if DistStr == \"Bernoulli\":\n",
    "            for k in self.K:\n",
    "                X_k = X[y == k, :]\n",
    "                N_k, D = X_k.shape\n",
    "                \n",
    "                self.likelihoods[k] = {\"P\" : N_k/len(x)}\n",
    "                self.priors[k] = len(X_k)/len(X)\n",
    "    \n",
    "    def predict(self, X, DistStr):\n",
    "        \n",
    "        N, D = X.shape\n",
    "        \n",
    "        if DistStr == \"Gauss\":\n",
    "            P_hat = np.zeros((N, len(self.K)))\n",
    "            \n",
    "            for k, l in self.likelihoods.items():\n",
    "                P_hat[:,k] = mvn.logpdf(X, l['mean'],l['cov']) + np.log(self.priors[k])\n",
    "        \n",
    "            return(P_hat.argmax(axis = 1))\n",
    "        \n",
    "        \n",
    "        if DistStr == \"Multinomial\":\n",
    "            P_hat = np.zeros((N, len(self.K)))\n",
    "            \n",
    "            for k, l in self.likelihoods.items():\n",
    "                #P_hat[:,k] = mlvn.logpdf(X, l['N'],l['P']) + np.log(self.priors[k])\n",
    "                P_hat[:,k] = mvn.logpdf(X, l['N'],l['P']) + np.log(self.priors[k])\n",
    "        \n",
    "            return(P_hat.argmax(axis = 1))\n",
    "                \n",
    "                \n",
    "        if DistStr == \"Bernoulli\":\n",
    "            P_hat = np.zeros((N, len(self.K)))\n",
    "            \n",
    "            for k, l in self.likelihoods.items():\n",
    "                # REWRITE THIS CORRECTLY\n",
    "                P_hat[:,k] = brn.logpdf(X, l['P']) + np.log(self.priors[k])\n",
    "            return(P_hat.argmax(axis = 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definition \n",
    "def accuracy(y,y_hat):\n",
    "    return np.mean(y == y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('xor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.to_numpy()\n",
    "y = X[:,-1] # storing just the labels\n",
    "X = X[:,:-1] # Dropping the the last column, AKA the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GenBayes()\n",
    "# gm = GenBayes()\n",
    "# gb = GenBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.fit(X, y,'Gauss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = g.predict(X, 'Gauss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Gaussian Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussNB():\n",
    "        \n",
    "    def fit(self, X, y, epsilon = 1e-2):\n",
    "        self.likelihoods = dict() # \n",
    "        self.priors = dict() #\n",
    "        \n",
    "        self.K = set(y.astype(int)) #\n",
    "        \n",
    "        for k in self.K:\n",
    "            \n",
    "            X_k  = X[y==k,:] \n",
    "            self.likelihoods[k] = {\"mean\" : X_k.mean(axis=0), \"cov\": X_k.var(axis = 0) + epsilon } \n",
    "            self.priors[k] = len(X_k)/len(X)  #        \n",
    "\n",
    "    def predictNB(self, X):\n",
    "        \n",
    "        N,D = X.shape \n",
    "        P_hat = np.zeros((N,len(self.K))) \n",
    "        \n",
    "        for k, l in self.likelihoods.items():\n",
    "            P_hat[:,k] = mvn.logpdf(X,l[\"mean\"], l[\"cov\"]) + np.log(self.priors[k])\n",
    "            \n",
    "        return P_hat.argmax(axis=1) # \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussBayes(): # not naive \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, epsilon = 1e-2):\n",
    "\n",
    "        \n",
    "        self.likihoods = dict() # \n",
    "        self.priors = dict() #     \n",
    "        self.K = set(y.astype(int)) # \n",
    "        \n",
    "        for k in self.K:   \n",
    "            X_k = X[y==k,:]\n",
    "            N_k, D = X_k.shape\n",
    "            mu_k = X_k.mean(axis = 0)\n",
    "            self.likihoods[k] = {\"mean\" : X_k.mean(axis=0), \"cov\": (1/(N_k - 1))*np.matmul((X_k - mu_k).T, X_k - mu_k) + epsilon*np.identity(D) } \n",
    "            # when multiplying a matrix by its transpose is what? What does that do\n",
    "            self.priors[k] = len(X_k)/len(X)  \n",
    "        \n",
    "    \n",
    "    # Now we get to the prediction\n",
    "    def predictGB(self, X):\n",
    "        N,D = X.shape # \n",
    "        P_hat = np.zeros((N,len(self.K))) # this is a tuple\n",
    "        \n",
    "        for k, l in self.likihoods.items(): \n",
    " \n",
    "            P_hat[:,k] = mvn.logpdf(X,l[\"mean\"], l[\"cov\"]) + np.log(self.priors[k]) # getting a function that gives us the probablity density\n",
    "            \n",
    "        return P_hat.argmax(axis=1) # maximum postier estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernBayes():\n",
    "  def fit(self, X, y, epsilon = 1e-3):\n",
    "    N, D = X.shape\n",
    "    self.likelihoods = {}\n",
    "    self.priors = {}\n",
    "    self.K = set(y.astype(int))\n",
    "\n",
    "    for k in self.K:\n",
    "      X_k = X[y==k,:]\n",
    "      p = (sum(X_k)+1) / (len(X_k)+2)\n",
    "      self.likelihoods[k] = {'mean': p, 'cov': p * (1 - p) + epsilon}\n",
    "      self.priors[k] = len(X_k)/len(X)\n",
    "\n",
    "  def predict(self, X):\n",
    "    N, D = X.shape\n",
    "    P_hat = np.zeros((N, len(self.K)))\n",
    "\n",
    "    for k,l in self.likelihoods.items():\n",
    "      # Using the Bernoulli funtion/formula. Trick is to get the matrices/vectors to go from mxn to a 1x1 number for each k value.\n",
    "      P_hat[:,k] = np.log(self.priors[k]) + np.matmul(X, np.log(l['mean'])) + np.matmul((1 - X), np.log(abs(1-l['mean'])))\n",
    "\n",
    "    return P_hat.argmax(axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussMB(): # not naive \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, epsilon = 1e-3):\n",
    "\n",
    "        \n",
    "        self.likihoods = dict() # \n",
    "        self.priors = dict() #     \n",
    "        self.K = set(y.astype(int)) # \n",
    "        \n",
    "        for k in self.K:\n",
    "            X_k = X[y == k,:] # All the values in class k\n",
    "            N_k, D = X_k.shape # N_k number of observations, D is number of features\n",
    "            mu_k = X_k.mean(axis=0)\n",
    "                \n",
    "            N = len(X)\n",
    "            self.likihoods[k] = {\"N\" : N, \"P\" : sum(N_k/len(X))}\n",
    "            self.priors[k] = len(X_k)/len(X)  \n",
    "        \n",
    "    \n",
    "    # Now we get to the prediction\n",
    "    def predictMB(self, X):\n",
    "        N,D = X.shape # \n",
    "        P_hat = np.zeros((N, len(self.K)))\n",
    "            \n",
    "        for k, l in self.likelihoods.items():\n",
    "            P_hat[:,k] = mlvn.logpmf(X, l['N'],l['P']) + np.log(self.priors[k])\n",
    "            #P_hat[:,k] = mvn.logpdf(X, l['N'],l['P']) + np.log(self.priors[k])\n",
    "        \n",
    "        return(P_hat.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K - Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNB():\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def predictKNN(self,X, K, epsilon = 1e-3):\n",
    "        N = len(X) # has no self, this was the X it was given in the parameters, self.X and X are different variables\n",
    "        y_hat = np.zeros(N)\n",
    "        \n",
    "        for i in range(N): # we are going through every single point. So thats a good thing or bad thing\n",
    "            #   use cases... wanted to find out all the members of a neighborhood \n",
    "            #   use cases... have map of the US that is built from lights that satellites can see, and we want to map the cities using that light\n",
    "            #   use cases... a model that mapped the disbursion of the dieses of the rona using the light map model from satellites \n",
    "            dist2 = np.sum((self.X - X[i])**2, axis=1) # by substracting from the ith member of x, we are getting the distance of each\n",
    "            idxt = np.argsort(dist2)[:K] # going to sort to each based on the distance \n",
    "            gamma_K = 1/(np.sqrt(dist2[idxt])+ epsilon) # we add epislon to avoid division by 0\n",
    "            y_hat[i] =  np.bincount(self.y[idxt], weights = gamma_K).argmax() # we want the smallest gamma_k\n",
    "            # taking the biggest chunks and summing them\n",
    "            # getting the probability that \n",
    "            # bincount produces the probability\n",
    "            # the argmax gives the actual value of the class, the maximum class\n",
    "            # bincount adds chunks, we're getting the distances of the sections we're adding\n",
    "            # \n",
    "        return  y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnbA_0 = accuracy(y_test[y_test == 0],y_hat[y_test == 0])\n",
    "gnbA_1 = accuracy(y_test[y_test == 1],y_hat[y_test == 1])\n",
    "gnbA_2 = accuracy(y_test[y_test == 2],y_hat[y_test == 2])\n",
    "gnbA_3 = accuracy(y_test[y_test == 3],y_hat[y_test == 3])\n",
    "gnbA_4 = accuracy(y_test[y_test == 4],y_hat[y_test == 4])\n",
    "gnbA_5 = accuracy(y_test[y_test == 5],y_hat[y_test == 5])\n",
    "gnbA_6 = accuracy(y_test[y_test == 6],y_hat[y_test == 6])\n",
    "gnbA_7 = accuracy(y_test[y_test == 7],y_hat[y_test == 7])\n",
    "gnbA_8 = accuracy(y_test[y_test == 8],y_hat[y_test == 8])\n",
    "gnbA_9 = accuracy(y_test[y_test == 9],y_hat[y_test == 9])\n",
    "\n",
    "gnb_A = [gnbA_0, gnbA_1, gnbA_2, gnbA_3, gnbA_4, gnbA_5, gnbA_6, gnbA_7, gnbA_8, gnbA_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
